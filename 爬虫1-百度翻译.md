# 爬虫1-百度翻译
>我的第一个爬虫（失败）
## 尝试
学完了小甲鱼的爬虫教程1、2之后，为了避免只是把代码打一编，所以选择了爬百度翻译（pandown事发让我毫无负罪感，而且本来教程就是2013的，这时候有道翻译据说已经反爬虫了，虽然现在百度好像也是）

### 雏形
本来也没想百度又这么多方式阻碍，但是骑虎难下了，一开始只是按照方法将小甲鱼的代码 翻 译 到适应（自认为）适合百度翻译的方式

直接上代码
---
#### 雏形
```python
import urllib.request
import urllib.parse
import json
import requests

query=input('输入要翻译的中文：')


lanurl='https://fanyi.baidu.com/langdetect'
data={'query':query}

data=urllib.parse.urlencode(data).encode('utf-8')

res=urllib.request.urlopen(lanurl,data)

res=res.read()
res=res.decode('utf-8')

print(res)
res=json.loads(res)

print('#输入的语言为: %s #' % (res['lan']))
'''
以上为通过爬虫获取输入语种的代码块（写笔记时注，写代码时才懒得写这句）
'''

data={}
data['from']:res['lan']
data['to']: 'en'
data['query']:query
data['transtype']:'translang'
data['simple_means_flag']:'3'
data['sign']:'145037.447932'
data['token']:'2ea893dfdd1a3b1221842636aed0b004'
data['domain']:'common'
fanurl='https://fanyi.baidu.com/v2transapi'
res=urllib.request.Request(url=fanurl,data=data,method='POST')
response=urllib.request.urlopen('https://fanyi.baidu.com')

res=response.read().decode('utf-8')
response=json.loads(res)

print(response) ##先测试下能不能获取传回的字典（失败了，干脆一直就没想着读取字典里的结果）

```
---
以上代码编译成功（也许，我一切事情结了才来写的笔记，这是写的时候删减修改的，也不嫌麻烦），但是回来的时空字节，而且诡异的是还返回了200（这括号里的简直的日记（吐嘈（这输入法连吐嘈都没又（用的RIME，只有本地词库，靠自己开荒（括号的错，又扯远了（无限套娃好爽））））））

---
##### 错误返回值（只选取重要部分）
>json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (==char 0==)

你看括号果然是个神奇的东西，人们会把重要（简单）的东西放到括号里，简明扼要（其实就是给傻子读的）

**’char 0‘，无字节，百度啥也没给我（烦躁）**

---

# sign
然后聪明如我的我发现了’sign‘这个值，**翻译不同文本时，sign值不同，虽然把query（也就是要翻译的文本）固定，即sign值固定，仍然不行**，不过我还是想修复这个问题（强迫症啊）

于是去百度翻博客，得到解决方法，直觉告诉我这也是以后写爬虫的惯用手法

==通过网页的元素审查，搜索’sign‘==，看得眼花了后，就可以看到sign:(n)
然后暂停网页（就这么叫吧），==跳转到这个函数==，是==JS代码==（废话（至少以我的了解是废话（给自己保命））），然后把这段==代码（运算sign函数）复制==下来，直接用==py调用==（以后忘了就百度吧），然后遇到两个依赖的变量，同样方法取得，保存为JS文件，懒的话就直接放桌面了（.py也是（没错懒就是形容我的））

---
#### sign_get.js
```
function n(r, o) {
        for (var t = 0; t < o.length - 2; t += 3) {
            var a = o.charAt(t + 2);
            a = a >= "a" ? a.charCodeAt(0) - 87 : Number(a),
            a = "+" === o.charAt(t + 1) ? r >>> a : r << a,
            r = "+" === o.charAt(t) ? r + a & 4294967295 : r ^ a
        }
        return r
    }
    
var i = "320305.131321201"
function e(r) {
        var o = r.match(/[\uD800-\uDBFF][\uDC00-\uDFFF]/g);
        if (null === o) {
            var t = r.length;
            t > 30 && (r = "" + r.substr(0, 10) + r.substr(Math.floor(t / 2) - 5, 10) + r.substr(-10, 10))
        } else {
            for (var e = r.split(/[\uD800-\uDBFF][\uDC00-\uDFFF]/), C = 0, h = e.length, f = []; h > C; C++)
                "" !== e[C] && f.push.apply(f, a(e[C].split(""))),
                C !== h - 1 && f.push(o[C]);
            var g = f.length;
            g > 30 && (r = f.slice(0, 10).join("") + f.slice(Math.floor(g / 2) - 5, Math.floor(g / 2) + 5).join("") + f.slice(-10).join(""))
        }
        var u = void 0
          , l = "" + String.fromCharCode(103) + String.fromCharCode(116) + String.fromCharCode(107);
        u = null !== i ? i : (i = window[l] || "") || "";
        
        for (var d = u.split("."), m = Number(d[0]) || 0, s = Number(d[1]) || 0, S = [], c = 0, v = 0; v < r.length; v++) {
            var A = r.charCodeAt(v);
            128 > A ? S[c++] = A : (2048 > A ? S[c++] = A >> 6 | 192 : (55296 === (64512 & A) && v + 1 < r.length && 56320 === (64512 & r.charCodeAt(v + 1)) ? (A = 65536 + ((1023 & A) << 10) + (1023 & r.charCodeAt(++v)),
            S[c++] = A >> 18 | 240,
            S[c++] = A >> 12 & 63 | 128) : S[c++] = A >> 12 | 224,
            S[c++] = A >> 6 & 63 | 128),
            S[c++] = 63 & A | 128)
        }
        for (var p = m, F = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(97) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(54)), D = "" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(51) + ("" + String.fromCharCode(94) + String.fromCharCode(43) + String.fromCharCode(98)) + ("" + String.fromCharCode(43) + String.fromCharCode(45) + String.fromCharCode(102)), b = 0; b < S.length; b++)
            p += S[b],
            p = n(p, F);
        return p = n(p, D),
        p ^= s,
        0 > p && (p = (2147483647 & p) + 2147483648),
        p %= 1e6,
        p.toString() + "." + (p ^ m)
    }

```

嗯，就把它当一个库用吧（其实很想叫它黑箱），把query放进去就能出来sign了，嗯就这样

++**以后有需要直接复制着用**++

---

### API中的sign
后来直接用API（成功）时我算是了解sign了，不对，和上面的sign（XXX.XXX）不同，API的sign是个MD5值（py使用需要hashlib库，直接pip install hashlib）
![输入值（字典）](https://gss0.bdstatic.com/70cFfyinKgQIm2_p8IuM_a/daf/pic/item/f11f3a292df5e0fe7d6e190f536034a85edf724e.jpg)

==sign = md5(appid + q + salt(一个随机值) + secretKey)==

*（只是表达意思，不是什么语言的代码哈）*

下面是百度给出的sign生成方法：
---
#### 签名生成方法
签名是为了保证调用安全，使用MD5算法生成的一段字符串，生成的签名长度为 32位，签名中的英文字符均为小写格式

##### 生成方法：
- Step1. 将请求参数中的 APPID(appid)， 翻译query(q, 注意为UTF-8编码), 随机数(salt), 以及平台分配的密钥(可在管理控制台查看) 按照 appid+q+salt+密钥 的顺序拼接得到字符串1。
- Step2. 对字符串1做md5，得到32位小写的sign。
##### 注：
1. ==待翻译文本（q）需为UTF-8编码==
2.  在生成签名拼接 appid+q+salt+密钥 字符串时，q不需要做URL encode，在生成签名之后，发送HTTP请求之前才需要对要发送的待翻译文本字段q做URL encode
- 请求方式： 可使用==GET==或==POST==方式，如使用POST方式Content-Type请指定为：==application/x-www-form-urlencoded==
- 字符编码：==统一采用UTF-8编码格式==
---

# header
即==发送给服务器的报头==，一般写爬虫用来蒙混过服务器对你是不是人的检查，包括你的信息，比如 ==设备== ==系统== ==版本== ==浏览器== 我连==CPU==都好像看到过。

我一开始的header只有==user-agent==（即上面说的那些）。后来越来越多（都是百度的错）
---

~~看个乐就好了~~


```
header = {"accept": "*/*",
    "accept-encoding": "gzip, deflate, br",
    "accept-language": "zh-CN,zh;q=0.9",
    "content-length": "122",
    "content-type": "application/x-www-form-urlencoded; charset=UTF-8",
    "cookie": "BIDUPSID=626E42CC20C367EA1F7DF29CA1D0B923",
    "origin": "https://fanyi.baidu.com",
    "referer": "https://fanyi.baidu.com/",
    "user-agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36",
    "x-requested-with": "XMLHttpRequest"}
```


---
## 放弃百度爬虫
试了好多次还是不行，不经叹气，百度就是烦，坚决反对网盘（FLAG）但是还是把最后的代码放上来吧

---
#### 最后的挣扎
```py
import urllib.request
import urllib.parse
import json
import requests
import execjs


query=input('输入要翻译的中文：')

with open('baidu_sign_get.js', 'r', encoding='utf-8') as f:
    ctx = execjs.compile(f.read())
sign = ctx.call('e', query)
print('#sign:%s#'%sign)


lanurl='https://fanyi.baidu.com/langdetect'
data={'query':query}

data=urllib.parse.urlencode(data).encode('utf-8')

res=urllib.request.urlopen(lanurl,data)

res=res.read()
res=res.decode('utf-8')

print(res)
res=json.loads(res)

print('#输入的语言为: %s #' % (res['lan']))




fanurl='https://fanyi.baidu.com/v2transapi'
header = {"accept": "*/*",
    "accept-encoding": "gzip, deflate, br",
    "accept-language": "zh-CN,zh;q=0.9",
    "content-length": "122",
    "content-type": "application/x-www-form-urlencoded; charset=UTF-8",
    "cookie": "BIDUPSID=626E42CC20C367EA1F7DF29CA1D0B923",  # 可以先将cookie信息复制sublime 避免 pycharm 出现乱序
    "origin": "https://fanyi.baidu.com",
    "referer": "https://fanyi.baidu.com/",
    "user-agent": "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36",
    "x-requested-with": "XMLHttpRequest"}

data={}
data['from']:res['lan']
data['to']: 'en'
data['query']:query
data['transtype']:'translang'
data['simple_means_flag']:'3'
data['sign']:sign
data['token']:'2ea893dfdd1a3b1221842636aed0b004'
data['domain']:'common'

data=urllib.parse.urlencode(data).encode('utf-8')

res=requests.post(url=fanurl,data=data,headers=header)
##res=urllib.request.Request(url=fanurl,data=data,method='POST')
print(type(res))
print(res)
#response=urllib.request.urlopen('https://fanyi.baidu.com')

#res=response.read().decode('utf-8')

response=res.content.decode()


response_1=json.loads(response)

print(response) 
```
#### 结果
不知道哪的数据包大了（也许）？反正很慢才出结(错)果(误)
>Response [500]

>raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)

## 开始用百度API
咳咳，这只是试试，知己知彼而已，我心里还是向着爬虫的，API就是百度拿来赚钱的（虽然无可厚非，但是网盘德行让我迁怒）

- [**百度官方的开发者文档**](https://api.fanyi.baidu.com/doc/21)


#### 输入参数
![image](https://gss0.bdstatic.com/70cFfyinKgQIm2_p8IuM_a/daf/pic/item/f11f3a292df5e0fe7d6e190f536034a85edf724e.jpg)

#### 输出参数
![image](https://gss0.bdstatic.com/70cFfyinKgQIm2_p8IuM_a/daf/pic/item/c8177f3e6709c93dc34fc791903df8dcd1005413.jpg)

#### 语言对照表
![image](https://gss0.bdstatic.com/70cFfyinKgQIm2_p8IuM_a/daf/pic/item/91ef76c6a7efce1b1fde01aca051f3deb58f65db.jpg)

#### 错误代码
![image](https://gss0.bdstatic.com/70cFfyinKgQIm2_p8IuM_a/daf/pic/item/359b033b5bb5c9ea5ef2cb85da39b6003af3b357.jpg)

- [==py示例下载==](https://fanyiapp.bj.bcebos.com/api/demo/BaiduTransAPI-forPython3.py.zip)（其他的编程语言在上面给出链接页面有，在靠下的位置）

虽然万般不愿，但是我还是把demo修改了一下，还是能用的（真香）

----------------------------end-------------------------------------

ps：（写笔记真麻烦，要是有这时间，我都能写另一个爬虫了）